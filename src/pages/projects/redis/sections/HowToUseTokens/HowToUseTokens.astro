---
import Heading from "@/components/Articles/Heading.astro";
import Keyword from "@/components/Articles/Keyword.astro";
import LexerNext from "./LexerNext.astro";
import LexerDefinition from "./LexerDefinition.astro";
import LexerInitialization from "./LexerInitialization.astro";
import LexerNextToken from "./LexerNextToken.astro";
import LexerFirstTokens from "./LexerFirstTokens.astro";
import LexerUtilities from "./LexerUtilities.astro";
import FinalNextToken from "./FinalNextToken.astro";
import LookupIdent from "./LookupIdent.astro";
---

<Heading id="how-to-use-tokens">
  How to use our Tokens: Building a Lexer
</Heading>

<p>
  To transform raw input like <Keyword word="SET username Alejandro" /> into a predictable
  stream of tokens, we need a <Keyword word="Lexer" /> (often called a <Keyword
    word="Scanner"
  />).
</p>

<p>
  If Tokens are the nouns of our language, the Lexer is the eye that identifies
  them. It scans the input text and converts it into a structured stream of data
  that our <Keyword word="Parser" /> can actually understand.
</p>

<h4 class="underline">The Engine Room: Key Lexer Mechanics</h4>
<p>
  To navigate through strings efficiently, our Lexer tracks its internal state
  using three specific fields:
</p>

<ol>
  <li>
    <Keyword word="pos" />: The starting index of the character we are currently
    examining.
  </li>

  <li>
    <Keyword word="readPos" />: Our "look-ahead" pointer. It sits one step ahead
    of pos to see what’s coming next (crucial for detecting <Keyword
      word="\\r\\n"
    />
    sequences).
  </li>

  <li>
    <Keyword word="ch" />: The actual byte value at our current position.
  </li>
</ol>

<p>
  By maintaining these two pointers, we can perform substring extraction with
  zero memory waste. To isolate a word like <Keyword word="userName" />, we
  simply "slice" the input string: <Keyword word="input[pos:readPos]" />
</p>

<LexerDefinition />

<h4 class="underline">The Constructor: Initializing the API</h4>

<p>
  To make our <Keyword word="Lexer" /> a reliable API, we have to create some <strong
    >helper functions first</strong
  >.
</p>

<ol>
  <li>
    <Keyword word="trimInput()" />: This helper removes trailing whitespaces to
    the right of the command. This prevents the Lexer from processing "ghost"
    characters at the end of a line.
  </li>

  <li>
    The <Keyword word="New()" /> constructor initializes the <Keyword
      word="lexer"
    /> by calling <Keyword word="next()" />, which handles
    character-by-character advancement through the input string. Here’s how it
    works:
  </li>
</ol>

<LexerInitialization />

<p>
  Now that we have our <Keyword word="New()" /> constructor, we can define <Keyword
    word="next()"
  />.
</p>

<LexerNext />

<p>
  The <Keyword word="next()" /> method is the internal engine that drives character-by-character
  advancement through the input string. It handles <strong
    >two critical cases:</strong
  >
</p>

<ol>
  <li>
    <strong>End of File (EOF):</strong> If <Keyword word="readPos" /> exceeds the
    input length, <Keyword word="ch" /> is set to <Keyword word="token.EOF" /> (the
    ASCII Null character). This signals that the input string has been fully <strong
      >consumed</strong
    >.
  </li>
  <li>
    <strong>Advancing:</strong> Otherwise, <Keyword word="ch" /> is updated to the
    character at <Keyword word="readPos" />. <Keyword word="pos" /> then catches
    up to <Keyword word="readPos" />, and <Keyword word="readPos" /> increments to
    <a href="#lexer-animation">"scout" the next index.</a>
  </li>
</ol>

<p>
  With character advancement handled by <Keyword word="next()" />, we'll now
  implement <Keyword word="NextToken()" /> to scan and return structured tokens from
  the input.
</p>

<p>
  <strong>Note:</strong> If you review the <a
    href="https://github.com/dyxgou/redis/blob/main/pkg/serializer/serializer.go"
    target="_blank"
    rel="noopener noreferrer">code implementation</a
  >, you'll notice that I have skipped some code. This is because the Lexer is
  used in both the <Keyword word="Serializer" /> and <Keyword word="Parser" />.
  However, the ideal approach is to create separate implementations for them,
  since this allows you to leverage the full potential of RESP.
</p>

<LexerNextToken />

<p>
  The first thing you can see is that the <Keyword word="NextToken()" /> returns
  a <Keyword word="token.Token" /> we have defined <a href="#whats-a-token"
    >here.</a
  >
  <br />
  Also this method performs <strong>three key</strong> operations:
</p>

<ol>
  <li>
    If <Keyword word="l.ch" /> is <Keyword word="token.EOF" />, this indicates
    that the input string has been fully consumed. In this case, an empty <Keyword
      word="Token"
    />
    with its <Keyword word="Kind" /> set to <Keyword word="token.EOF" /> is returned.
  </li>
  <li>
    Skips any leading <strong>white space</strong> characters (e.g., spaces, tabs
    <Keyword word="\\t" />, newlines <Keyword word="\\n" />).
  </li>

  <li>
    Using the <Keyword word="peekChar()" /> function which returns us the <strong
      >next character</strong
    > without advancing <Keyword word="readPos" />, we can check if the current
    character is <Keyword word="\\r" /> and the <strong>next character</strong> is
    <Keyword word="\\n" /> (forming <Keyword word="\\r\\n" /> known as CRLF), which
    acts as a <Keyword word="Token" /> separator, so a CRLF token is returned.
  </li>
</ol>

<LexerFirstTokens />

<p>
  Now, we need to be able to create <Keyword word="Tokens" /> for like <Keyword
    word="SET"
  />, <Keyword word="userName" />, and <Keyword word="Alejandro" />. As you may
  have noticed, what these all have in common is that they are <strong
    >composed of letters.</strong
  >
  <br />
  To handle the identification of keywords and identifiers more effectively, let's
  create two utility functions.
</p>

<LexerUtilities />

<p>
  This is a simplified implementation, if you wanna see it whole i recommend you
  to check the implementation details.
  <br />
  For now let's see what is happening here.
</p>

<ol>
  <li>
    <Keyword word="isLetter" />: This function checks if the given character (<Keyword
      word="ch"
    />) is a letter
    <Keyword word="(a-z, A-Z)" /> or an underscore (<Keyword word="_" />).
  </li>
  <li>
    <Keyword word="readIdent" />: This method reads an entire <strong
      >identifier
    </strong> (a sequence of letters and underscores) from the input
  </li>
</ol>

<p>
  Now that we have implemented these utility functions, let's finally examine
  the second part of the <Keyword word="NextToken()" /> implementation.
</p>

<FinalNextToken />

<p>
  <strong>NOTE:</strong> Implementing Numbers is pretty similar to the <strong
    >characters implementation</strong
  >.
</p>

<p>
  This completes the core logic for identifying keywords and identifiers. If the
  lexer doesn't recognize any token (including <Keyword word="keywords" />, <Keyword
    word="identifiers"
  /> or <Keyword word="numbers" />), it defaults to returning a <Keyword
    word="token.ILLEGAL"
  />. As you may have noticed, we haven't yet defined <Keyword
    word="token.LookupIdent"
  />. Let's do that now.
</p>

<LookupIdent />

<p>
  As you can see this function is pretty straight forward, and with this we have
  finished our <strong>Lexer</strong>.
  <br />
  So, after all of this work we can finally <strong>implement a Parser</strong>.
</p>
